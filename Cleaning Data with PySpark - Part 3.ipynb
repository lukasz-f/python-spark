{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b05df18-929c-45cc-ac49-6c072756357c",
   "metadata": {},
   "source": [
    "# Cleaning Data with PySpark - Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d4d5e4-3057-4f86-82c3-bf063dc9d778",
   "metadata": {},
   "source": [
    "## Improving Performance\n",
    "Improve data cleaning tasks by increasing performance or reducing resource requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b93a7fcc-d393-45c0-be09-c137a0cd6962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET = 'driven-actor-210609'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc9938e1-7d66-40da-8d3c-b13df1ec8e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5caad7f4-864d-4d2e-85c1-02e99f86aff0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/13 12:17:25 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/03/13 12:17:26 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/03/13 12:17:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/03/13 12:17:26 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421402e-056d-485b-b456-c08abdc7bf31",
   "metadata": {},
   "source": [
    "### Caching a DataFrame\n",
    "You've been assigned a task that requires running several analysis operations on a DataFrame. You've learned that caching can improve performance when reusing DataFrames and would like to implement it.\n",
    "\n",
    "You'll be working with a new dataset consisting of airline departure information. It may have repetitive data and will need to be de-duplicated.\n",
    "\n",
    "The DataFrame `departures_df` is defined, but no actions have been performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f3a49a-c491-420b-b519-91bac64ebbc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "file_path = f'gs://{BUCKET}/pyspark/datasets/AA_DFW_2017_Departures.csv.gz'\n",
    "# file_path = 'datasets/AA_DFW_2017_Departures.csv.gz'\n",
    "\n",
    "departures_df = spark.read.format('csv').options(Header=True).load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45599b66-6da9-4961-bc78-40fb59f913cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 139358 rows took 24.519457 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:====================================================>(985 + 3) / 1000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 139358 rows again took 5.385797 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Add caching to the unique rows in departures_df\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "# Count the unique rows in departures_df, noting how long the operation takes\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571b37b3-3ed3-4c60-9c06-947e33a498f3",
   "metadata": {},
   "source": [
    "### Removing a DataFrame from cache\n",
    "You've finished the analysis tasks with the departures_df DataFrame, but have some other processing to do. You'd like to remove the DataFrame from the cache to prevent any excess memory usage on your cluster.\n",
    "\n",
    "The DataFrame `departures_df` is defined and has already been cached for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "745c90d9-2d25-47ac-9ec4-0268ef2736ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is departures_df cached?: True\n",
      "Removing departures_df from cache\n",
      "Is departures_df cached?: False\n"
     ]
    }
   ],
   "source": [
    "# Determine if departures_df is in the cache\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
    "print(\"Removing departures_df from cache\")\n",
    "\n",
    "# Remove departures_df from the cache\n",
    "departures_df.unpersist()\n",
    "\n",
    "# Check the cache status again\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a60407d-6cfb-4f37-bdd2-13e0c8d1549e",
   "metadata": {},
   "source": [
    "### Object splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa38a5e-c7da-4e00-9c72-c76a73722015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split large file departures_full.txt to files with 10000 rows per file and chunk prefix. e.g: chunk-00, chunk-01, chunk-02, ...\n",
    "!split -l 10000 -d departures_full.txt chunk-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428bef4e-9144-4e16-aa97-8ba5a65acdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write large file as parquet data format to improve performance\n",
    "df_csv = spark.read.csv('departures_full.txt.gz')\n",
    "df_csv.write.parquet('departures.parquet')\n",
    "df = spark.read.parquet('departures.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b4ea2e-6a0c-4000-8c49-48c483dafa5a",
   "metadata": {},
   "source": [
    "### File import performance\n",
    "You've been given a large set of data to import into a Spark DataFrame. You'd like to test the difference in import speed by splitting up the file.\n",
    "\n",
    "You have two types of files available: `departures_full.txt.gz` and `departures_xxx.txt.gz` where xxx is 000 - 013. The same number of rows is split between each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c160716-d34b-44b3-97ff-dc667b9c1be3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_file_path = f'gs://{BUCKET}/pyspark/datasets/AA_DFW_2017_Departures.csv.gz'\n",
    "split_file_path = f'gs://{BUCKET}/pyspark/datasets/AA_DFW_2017_Departures_???.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3e26975-da74-4034-a125-024d5621eeb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in full DataFrame:\t139359\n",
      "Time to run: 0.606498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in split DataFrame:\t139359\n",
      "Time to run: 0.961519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import the full and split files into DataFrames\n",
    "full_df = spark.read.csv(full_file_path)\n",
    "split_df = spark.read.csv(split_file_path)\n",
    "\n",
    "# Print the count and run time for each DataFrame\n",
    "start_time_a = time.time()\n",
    "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
    "\n",
    "start_time_b = time.time()\n",
    "print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be012ca2-cd61-40c2-a4d2-3840638a44c8",
   "metadata": {},
   "source": [
    "### Reading Spark configurations\n",
    "You've recently configured a cluster via a cloud provider. Your only access is via the command shell or your python code. You'd like to verify some Spark settings to validate the configuration of the cluster.\n",
    "\n",
    "The `spark` object is available for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "307a36d0-70cf-4cbe-bd20-935b3d9495cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark-shell\n",
      "Driver TCP port: 35173\n",
      "Number of partitions: 1000\n"
     ]
    }
   ],
   "source": [
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get('spark.app.name')\n",
    "\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2973c070-2386-47fc-90a8-3780568bd51d",
   "metadata": {},
   "source": [
    "### Writing Spark configurations\n",
    "Now that you've reviewed some of the Spark configurations on your cluster, you want to modify some of the settings to tune Spark to your needs. You'll import some data to review that your changes have affected the cluster.\n",
    "\n",
    "The spark configuration is initially set to the default value of 200 partitions.\n",
    "\n",
    "The `spark` object is available for use. A file named `departures.txt.gz` is available for import. An initial DataFrame containing the distinct rows from `departures.txt.gz` is available as `departures_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99319ab2-f632-499a-8cae-9271bc532407",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'gs://{BUCKET}/pyspark/datasets/AA_DFW_2017_Departures.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58573c37-283e-4dec-ac3f-d1c4fe7d0691",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition count before change: 2\n",
      "Partition count after change: 1\n"
     ]
    }
   ],
   "source": [
    "departures_df = spark.read.csv(file_path).distinct()\n",
    "\n",
    "# Store the number of partitions in variable\n",
    "before = departures_df.rdd.getNumPartitions()\n",
    "\n",
    "# Configure Spark to use 500 partitions\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 10)\n",
    "\n",
    "# Recreate the DataFrame using the departures data file\n",
    "departures_df = spark.read.csv(file_path).distinct()\n",
    "\n",
    "# Print the number of partitions for each instance\n",
    "print(\"Partition count before change: %d\" % before)\n",
    "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2627741-c067-4edf-967c-f914b41f8362",
   "metadata": {},
   "source": [
    "### Normal joins\n",
    "You've been given two DataFrames to combine into a single useful DataFrame. Your first task is to combine the DataFrames normally and view the execution plan.\n",
    "\n",
    "The DataFrames `flights_df` and `airports_df` are available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fffa5432-3b7e-429a-bb15-dfdbaec9167f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|01/01/2018       |0005         |HNL                |498                          |\n",
      "|01/01/2018       |0007         |OGG                |501                          |\n",
      "|01/01/2018       |0043         |DTW                |0                            |\n",
      "|01/01/2018       |0051         |STL                |100                          |\n",
      "|01/01/2018       |0075         |DCA                |147                          |\n",
      "|01/01/2018       |0096         |STL                |92                           |\n",
      "|01/01/2018       |0103         |SJC                |227                          |\n",
      "|01/01/2018       |0119         |OGG                |517                          |\n",
      "|01/01/2018       |0123         |HNL                |489                          |\n",
      "|01/01/2018       |0128         |MCO                |141                          |\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_file_path = f'gs://{BUCKET}/pyspark/datasets/AA_DFW_2018_Departures.csv.gz'\n",
    "flights_df = spark.read.options(Header=True).csv(flights_file_path)\n",
    "flights_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8600f07f-9fe8-4873-9635-2a394e9d746e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+----+\n",
      "|AIRPORTNAME                                |IATA|\n",
      "+-------------------------------------------+----+\n",
      "|Goroka Airport                             |GKA |\n",
      "|Madang Airport                             |MAG |\n",
      "|Mount Hagen Kagamuga Airport               |HGU |\n",
      "|Nadzab Airport                             |LAE |\n",
      "|Port Moresby Jacksons International Airport|POM |\n",
      "|Wewak International Airport                |WWK |\n",
      "|Narsarsuaq Airport                         |UAK |\n",
      "|Godthaab / Nuuk Airport                    |GOH |\n",
      "|Kangerlussuaq Airport                      |SFJ |\n",
      "|Thule Air Base                             |THU |\n",
      "+-------------------------------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports_file_path = f'gs://{BUCKET}/pyspark/datasets/airportnames.txt.gz'\n",
    "airports_df = spark.read.options(Header=True).csv(airports_file_path)\n",
    "airports_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b64335a8-15ca-41ba-8de6-70654829b722",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [Destination Airport#874], [IATA#920], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(Destination Airport#874)\n",
      "   :  +- FileScan csv [Date (MM/DD/YYYY)#872,Flight Number#873,Destination Airport#874,Actual elapsed time (Minutes)#875] Batched: false, DataFilters: [isnotnull(Destination Airport#874)], Format: CSV, Location: InMemoryFileIndex(1 paths)[gs://driven-actor-210609/pyspark/datasets/AA_DFW_2018_Departures.csv.gz], PartitionFilters: [], PushedFilters: [IsNotNull(Destination Airport)], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:string,Destination Airport:string,Actual elapsed ti...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, false]),false), [plan_id=1461]\n",
      "      +- Filter isnotnull(IATA#920)\n",
      "         +- FileScan csv [AIRPORTNAME#919,IATA#920] Batched: false, DataFilters: [isnotnull(IATA#920)], Format: CSV, Location: InMemoryFileIndex(1 paths)[gs://driven-actor-210609/pyspark/datasets/airportnames.txt.gz], PartitionFilters: [], PushedFilters: [IsNotNull(IATA)], ReadSchema: struct<AIRPORTNAME:string,IATA:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the flights_df and aiports_df DataFrames\n",
    "normal_df = flights_df.join(airports_df, \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# Show the query plan\n",
    "normal_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d138fa-b4c0-446a-b21d-aa5b14bd44be",
   "metadata": {},
   "source": [
    "### Using broadcasting on Spark joins\n",
    "Remember that table joins in Spark are split between the cluster workers. If the data is not local, various shuffle operations are required and can have a negative impact on performance. Instead, we're going to use Spark's `broadcast` operations to give each node a copy of the specified data.\n",
    "\n",
    "A couple tips:\n",
    "- Broadcast the smaller DataFrame. The larger the DataFrame, the more time required to transfer to the worker nodes.\n",
    "- On small DataFrames, it may be better skip broadcasting and let Spark figure out any optimization on its own.\n",
    "- If you look at the query execution plan, a broadcastHashJoin indicates you've successfully configured broadcasting.\n",
    "\n",
    "The DataFrames `flights_df` and `airports_df` are available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d0d7a7f-14d4-436c-8f4e-1fa9741a08b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [Destination Airport#874], [IATA#920], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(Destination Airport#874)\n",
      "   :  +- FileScan csv [Date (MM/DD/YYYY)#872,Flight Number#873,Destination Airport#874,Actual elapsed time (Minutes)#875] Batched: false, DataFilters: [isnotnull(Destination Airport#874)], Format: CSV, Location: InMemoryFileIndex(1 paths)[gs://driven-actor-210609/pyspark/datasets/AA_DFW_2018_Departures.csv.gz], PartitionFilters: [], PushedFilters: [IsNotNull(Destination Airport)], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:string,Destination Airport:string,Actual elapsed ti...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, false]),false), [plan_id=1484]\n",
      "      +- Filter isnotnull(IATA#920)\n",
      "         +- FileScan csv [AIRPORTNAME#919,IATA#920] Batched: false, DataFilters: [isnotnull(IATA#920)], Format: CSV, Location: InMemoryFileIndex(1 paths)[gs://driven-actor-210609/pyspark/datasets/airportnames.txt.gz], PartitionFilters: [], PushedFilters: [IsNotNull(IATA)], ReadSchema: struct<AIRPORTNAME:string,IATA:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the broadcast method from pyspark.sql.functions\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Join the flights_df and airports_df DataFrames using broadcasting\n",
    "broadcast_df = flights_df.join(broadcast(airports_df), \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# Show the query plan and compare against the original\n",
    "broadcast_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31074d6-7cf0-44f1-ab40-ac36b0b7d4de",
   "metadata": {},
   "source": [
    "### Comparing broadcast vs normal joins\n",
    "You've created two types of joins, normal and broadcasted. Now your manager would like to know what the performance improvement is by using Spark optimizations. If the results are promising, you'll be given more opportunity to tweak the Spark setup as needed.\n",
    "\n",
    "Your DataFrames `normal_df` and `broadcast_df` are available for your use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2231ea-9ebd-40a6-bd67-ebdb6d11d0de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Count the number of rows in the normal DataFrame\n",
    "normal_count = normal_df.count()\n",
    "normal_duration = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "# Count the number of rows in the broadcast DataFrame\n",
    "broadcast_count = broadcast_df.count()\n",
    "broadcast_duration = time.time() - start_time\n",
    "\n",
    "# Print the counts and the duration of the tests\n",
    "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
    "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d982a3e-d7ed-4da5-9162-d6e19106edd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}