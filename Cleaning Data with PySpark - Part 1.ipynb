{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab92d1b-daf4-4f17-b8d0-e23717edf166",
   "metadata": {},
   "source": [
    "# Cleaning Data with PySpark - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b84f7-2fd8-45bc-a970-680afe58e5b2",
   "metadata": {},
   "source": [
    "## DataFrame details\n",
    "A review of DataFrame fundamentals and the importance of data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd96428a-10bb-4b4c-8fc2-76aa42ef4239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET = 'driven-actor-210609'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8ad9e8e-3e23-4a97-becc-3bd38fd94165",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1208aa3d-fd1d-4a66-9d26-550f92307a49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/12 20:54:52 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/03/12 20:54:52 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/03/12 20:54:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/03/12 20:54:53 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709e3d7e-d456-4800-876c-4181b1bd5996",
   "metadata": {},
   "source": [
    "### Defining a schema\n",
    "Creating a defined schema helps with data quality and import performance. As mentioned during the lesson, we'll create a simple schema to read in the following columns:\n",
    "- Name\n",
    "- Age\n",
    "- City\n",
    "\n",
    "The Name and City columns are StringType() and the Age column is an IntegerType()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176b77c-454a-4caa-a50a-9c6f99f5e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark.sql.types library\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define a new schema using the StructType method\n",
    "people_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('name', StringType(), False),\n",
    "  StructField('age', IntegerType(), False),\n",
    "  StructField('city', StringType(), False)\n",
    "])\n",
    "\n",
    "# Load data with specific schema\n",
    "people_df = spark.read.format('csv').load(name='rawdata.csv', schema=people_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de8465f-db8e-4b82-89ff-b28ddd658501",
   "metadata": {},
   "source": [
    "### Using lazy processing\n",
    "Lazy processing operations will usually return in about the same amount of time regardless of the actual quantity of data. Remember that this is due to Spark not performing any transformations until an action is requested.\n",
    "\n",
    "For this exercise, we'll be defining a Data Frame (`aa_dfw_df`) and add a couple transformations. Note the amount of time required for the transformations to complete when defined vs when the data is actually queried. These differences may be short, but they will be noticeable. When working with a full Spark cluster with larger quantities of data the difference will be more apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40a2842b-dae0-4d4f-9eb2-a48bd2d59339",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = f'gs://{BUCKET}/pyspark/datasets/AA_DFW_2018_Departures.csv.gz'\n",
    "# file_path = 'datasets/AA_DFW_2018_Departures.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f293dc95-5cbc-4da5-970e-3a735fdce49a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+-------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|airport|\n",
      "+-----------------+-------------+-------------------+-----------------------------+-------+\n",
      "|       01/01/2018|         0005|                HNL|                          498|    hnl|\n",
      "|       01/01/2018|         0007|                OGG|                          501|    ogg|\n",
      "|       01/01/2018|         0043|                DTW|                            0|    dtw|\n",
      "|       01/01/2018|         0051|                STL|                          100|    stl|\n",
      "|       01/01/2018|         0075|                DCA|                          147|    dca|\n",
      "|       01/01/2018|         0096|                STL|                           92|    stl|\n",
      "|       01/01/2018|         0103|                SJC|                          227|    sjc|\n",
      "|       01/01/2018|         0119|                OGG|                          517|    ogg|\n",
      "|       01/01/2018|         0123|                HNL|                          489|    hnl|\n",
      "|       01/01/2018|         0128|                MCO|                          141|    mco|\n",
      "|       01/01/2018|         0132|                EWR|                          201|    ewr|\n",
      "|       01/01/2018|         0140|                SJC|                          215|    sjc|\n",
      "|       01/01/2018|         0174|                RDU|                          140|    rdu|\n",
      "|       01/01/2018|         0190|                SAT|                           68|    sat|\n",
      "|       01/01/2018|         0200|                SFO|                          215|    sfo|\n",
      "|       01/01/2018|         0209|                MIA|                          169|    mia|\n",
      "|       01/01/2018|         0217|                LAS|                          178|    las|\n",
      "|       01/01/2018|         0229|                KOA|                          534|    koa|\n",
      "|       01/01/2018|         0244|                CVG|                          115|    cvg|\n",
      "|       01/01/2018|         0262|                MIA|                          159|    mia|\n",
      "+-----------------+-------------+-------------------+-----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load(file_path)\n",
    "\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda74afc-7612-44cd-9f93-bfdc1fb9a8b2",
   "metadata": {},
   "source": [
    "### Saving a DataFrame in Parquet format\n",
    "When working with Spark, you'll often start with CSV, JSON, or other data sources. This provides a lot of flexibility for the types of data to load, but it is not an optimal format for Spark. The `Parquet` format is a columnar data store, allowing Spark to use predicate pushdown. This means Spark will only process the data necessary to complete the operations you define versus reading the entire dataset. This gives Spark more flexibility in accessing the data and often drastically improves performance on large datasets.\n",
    "\n",
    "In this exercise, we're going to practice creating a new Parquet file and then process some data from it.\n",
    "\n",
    "The `spark` object and the `df1` and `df2` DataFrames have been setup for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c2317c1-71c9-4896-91d5-f143d8bca24f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path_2017 = f'gs://{BUCKET}/pyspark/datasets/AA_DFW_2017_Departures.csv.gz'\n",
    "df1 = spark.read.format('csv').options(Header=True).load(file_path_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c160937-7f0c-4dfe-a40a-6f795c09e357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path_2018 = f'gs://{BUCKET}/pyspark/datasets/AA_DFW_2018_Departures.csv.gz'\n",
    "df2 = spark.read.format('csv').options(Header=True).load(file_path_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c275af76-44fa-41aa-81c7-c6cee4f05287",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 Count: 139358\n",
      "df2 Count: 119910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259268\n"
     ]
    }
   ],
   "source": [
    "# View the row count of df1 and df2\n",
    "print(\"df1 Count: %d\" % df1.count())\n",
    "print(\"df2 Count: %d\" % df2.count())\n",
    "\n",
    "# Combine the DataFrames into one\n",
    "df3 = df1.union(df2)\n",
    "\n",
    "file_path_all = f'gs://{BUCKET}/pyspark/datasets/AA_DFW_ALL.parquet'\n",
    "# Save the df3 DataFrame in Parquet format\n",
    "df3.coalesce(1).write.parquet(file_path_all, mode='overwrite')\n",
    "\n",
    "# Read the Parquet file into a new DataFrame and run a count\n",
    "print(spark.read.parquet(file_path_all).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42cb35c-1f26-43de-9d36-d1a380b8581b",
   "metadata": {},
   "source": [
    "### SQL and Parquet\n",
    "Parquet files are perfect as a backing data store for SQL queries in Spark. While it is possible to run the same queries directly via Spark's Python functions, sometimes it's easier to run SQL queries alongside the Python options.\n",
    "\n",
    "For this example, we're going to read in the Parquet file we created in the last exercise and register it as a SQL table. Once registered, we'll run a quick query against the table (aka, the Parquet file).\n",
    "\n",
    "The `spark` object and the `AA_DFW_ALL.parquet` file are available for you automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba23e7cb-03fc-42fe-a4f2-386c5282aa54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path_all = f'gs://{BUCKET}/pyspark/datasets/AA_DFW_ALL.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71c41403-c2f6-4c80-899d-bb33da260079",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average flight time is: 151\n"
     ]
    }
   ],
   "source": [
    "# Read the Parquet file into flights_df\n",
    "flights_df = spark.read.options(Header=True).parquet(file_path_all)\n",
    "\n",
    "flights_df = flights_df.withColumnRenamed('Actual elapsed time (Minutes)', 'flight_duration')\n",
    "\n",
    "# Register the temp table\n",
    "flights_df.createOrReplaceTempView('flights')\n",
    "\n",
    "# Run a SQL query of the average flight duration\n",
    "avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]\n",
    "print('The average flight time is: %d' % avg_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e0dc26-af26-401b-8826-58f0900d7256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}