{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad5333ce-f130-4bd3-bc85-bd45dc578adb",
   "metadata": {},
   "source": [
    "# Cleaning Data with PySpark - Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9777f565-2e23-456a-814b-aed367eba4c6",
   "metadata": {},
   "source": [
    "## Complex processing and data pipelines\n",
    "Learn how to process complex real-world data using Spark and the basics of pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3653b184-7d78-4d36-9ede-0777647e2441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET = 'driven-actor-210609'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "badbb76f-2c66-44f8-b1ee-449e07690db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46746fc9-bd97-4a55-8713-0a5a98af29d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/13 15:13:45 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/03/13 15:13:45 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/03/13 15:13:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/03/13 15:13:46 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b72e4c4-3662-4969-91c5-9421215bf15e",
   "metadata": {},
   "source": [
    "### Quick pipeline\n",
    "Before you parse some more complex data, your manager would like to see a simple pipeline example including the basic steps. For this example, you'll want to ingest a data file, filter a few rows, add an ID column to it, then write it out as JSON data.\n",
    "\n",
    "The `spark` context is defined, along with the `pyspark.sql.functions` library being aliased as `F` as is customary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86576ebf-8eff-4aa6-8e99-4670369ed4d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = f'gs://{BUCKET}/pyspark/datasets/AA_DFW_2015_Departures.csv.gz'\n",
    "# file_path = 'datasets/AA_DFW_2017_Departures.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2fa78e-bed6-4a4f-8636-6b58a8f2a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data to a DataFrame\n",
    "departures_df = spark.read.csv(file_path, header=True)\n",
    "\n",
    "# Remove any duration of 0\n",
    "departures_df = departures_df.filter(departures_df['Actual elapsed time (Minutes)'] == 0)\n",
    "\n",
    "# Add an ID column\n",
    "departures_df = departures_df.withColumn('id', F.monotonically_increasing_id())\n",
    "\n",
    "# Write the file out to JSON format\n",
    "departures_df.write.json('output.json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ea2ec-9133-405b-8cfb-cf1e4f8303dc",
   "metadata": {},
   "source": [
    "### Removing commented lines\n",
    "Your boss would like you to perform some complex parsing on a new dataset. The data represents annotation data for the ImageNet dataset, but focusing specifically on dog breeds and identifying them in images. Before any actual analysis can occur, you'll need to clear out several components of invalid / incorrect data. The general schema of the document is unknown so you'd like to import the rows into a single column, allowing for quick analysis.\n",
    "\n",
    "To start, you need to remove all commented rows in the dataset.\n",
    "\n",
    "The `spark` context, and the base CSV file (`annotations.csv.gz`) are available for you to work with. The `col` function is also available for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbb19e40-4efd-4513-9600-ea297c61ea4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|_c0                                                               |\n",
      "+------------------------------------------------------------------+\n",
      "|025865917\\tn023521131_781                                         |\n",
      "|022684404\\tn029380957_9768                                        |\n",
      "|021267273\\tn022910760_9023                                        |\n",
      "|02110627\\tn02110627_12938\\t200\\t300\\taffenpinscher,0,9,173,298    |\n",
      "|02093754\\tn02093754_1148\\t500\\t378\\tBorder_terrier,73,127,341,335 |\n",
      "|%s\\t%s\\t800\\t600\\tShetland_sheepdog,124,87,576,514                |\n",
      "|023200662\\tn023050402_6068                                        |\n",
      "|028666219\\tn025734155_5660                                        |\n",
      "|02104029\\tn02104029_63\\t500\\t375\\tkuvasz,0,0,499,327              |\n",
      "|02111500\\tn02111500_5137\\t500\\t375\\tGreat_Pyrenees,124,225,403,374|\n",
      "+------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = f'gs://{BUCKET}/pyspark/datasets/annotations.csv.gz'\n",
    "# file_path = 'datasets/annotations.csv.gz'\n",
    "\n",
    "# Import the data to a DataFrame\n",
    "annotations_df = spark.read.csv(file_path, sep='|')\n",
    "\n",
    "annotations_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbbbfaad-01da-4ec8-b46a-28ddf08d03a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full count: 32794\n",
      "Comment count: 1416\n",
      "Remaining count: 31378\n"
     ]
    }
   ],
   "source": [
    "# Perform a row count\n",
    "full_count = annotations_df.count()\n",
    "\n",
    "# Count the number of rows beginning with '#'\n",
    "comment_count = annotations_df.filter(F.col('_c0').startswith('#')).count()\n",
    "\n",
    "# Import the file to a new DataFrame, without commented rows\n",
    "no_comments_df = spark.read.csv(file_path, sep='|', comment='#')\n",
    "\n",
    "# Count the new DataFrame and verify the difference is as expected\n",
    "no_comments_count = no_comments_df.count()\n",
    "print(\"Full count: %d\\nComment count: %d\\nRemaining count: %d\" % (full_count, comment_count, no_comments_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e8a43f-c04d-4a26-98ab-be670e9bb606",
   "metadata": {},
   "source": [
    "### Removing invalid rows\n",
    "Now that you've successfully removed the commented rows, you have received some information about the general format of the data. There should be at minimum 5 tab separated columns in the DataFrame. Remember that your original DataFrame only has a single column, so you'll need to split the data on the tab (`\\t`) characters.\n",
    "\n",
    "The DataFrame `annotations_df` is already available, with the commented rows removed. The `spark.sql.functions` library is available under the alias `F`. The initial number of rows available in the DataFrame is stored in the variable `initial_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f44ea8c-d1f0-4399-9265-86ff71747660",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial count: 31378\n",
      "Final count: 20580\n"
     ]
    }
   ],
   "source": [
    "annotations_df = no_comments_df\n",
    "initial_count = annotations_df.count()\n",
    "\n",
    "# Split _c0 on the tab character and store the list in a variable\n",
    "tmp_fields = F.split(annotations_df['_c0'], '\\t')\n",
    "\n",
    "# Create the colcount column on the DataFrame\n",
    "annotations_df = annotations_df.withColumn('colcount', F.size(tmp_fields))\n",
    "\n",
    "# Remove any rows containing fewer than 5 fields\n",
    "annotations_df_filtered = annotations_df.filter(~ (F.col('colcount') < 5))\n",
    "\n",
    "# Count the number of rows\n",
    "final_count = annotations_df_filtered.count()\n",
    "print(\"Initial count: %d\\nFinal count: %d\" % (initial_count, final_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d88ce4-f485-422e-b0d1-e847750b969e",
   "metadata": {},
   "source": [
    "### Splitting into columns\n",
    "You've cleaned up your data considerably by removing the invalid rows from the DataFrame. Now you want to perform some further transformations by generating specific meaningful columns based on the DataFrame content.\n",
    "\n",
    "You have the `spark` context and the latest version of the `annotations_df` DataFrame. `pyspark.sql.functions` is available under the alias `F`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75919efc-292a-4aef-beef-de1fb12018bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------+---------------+-----+------+--------------------+\n",
      "|                 _c0|colcount|  folder|       filename|width|height|          split_cols|\n",
      "+--------------------+--------+--------+---------------+-----+------+--------------------+\n",
      "|02110627\\tn021106...|       5|02110627|n02110627_12938|  200|   300|[02110627, n02110...|\n",
      "|02093754\\tn020937...|       5|02093754| n02093754_1148|  500|   378|[02093754, n02093...|\n",
      "|%s\\t%s\\t800\\t600\\...|       5|      %s|             %s|  800|   600|[%s, %s, 800, 600...|\n",
      "|02104029\\tn021040...|       5|02104029|   n02104029_63|  500|   375|[02104029, n02104...|\n",
      "|02111500\\tn021115...|       5|02111500| n02111500_5137|  500|   375|[02111500, n02111...|\n",
      "|02104365\\tn021043...|       5|02104365| n02104365_7518|  500|   333|[02104365, n02104...|\n",
      "|02105056\\tn021050...|       5|02105056| n02105056_2834|  500|   375|[02105056, n02105...|\n",
      "|02093647\\tn020936...|       5|02093647|  n02093647_541|  500|   333|[02093647, n02093...|\n",
      "|02098413\\tn020984...|       5|02098413| n02098413_1355|  500|   375|[02098413, n02098...|\n",
      "|02093859\\tn020938...|       5|02093859| n02093859_2309|  330|   500|[02093859, n02093...|\n",
      "+--------------------+--------+--------+---------------+-----+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "annotations_df = annotations_df_filtered\n",
    "\n",
    "# Split the content of _c0 on the tab character (aka, '\\t')\n",
    "split_cols = F.split(annotations_df['_c0'], '\\t')\n",
    "\n",
    "# Add the columns folder, filename, width, and height\n",
    "split_df = annotations_df.withColumn('folder', split_cols.getItem(0))\n",
    "split_df = split_df.withColumn('filename', split_cols.getItem(1))\n",
    "split_df = split_df.withColumn('width', split_cols.getItem(2))\n",
    "split_df = split_df.withColumn('height', split_cols.getItem(3))\n",
    "\n",
    "# Add split_cols as a column\n",
    "split_df = split_df.withColumn('split_cols', split_cols)\n",
    "\n",
    "split_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd9d4d3-b612-4869-94fa-c177f81de5a6",
   "metadata": {},
   "source": [
    "### Further parsing\n",
    "You've molded this dataset into a significantly different format than it was before, but there are still a few things left to do. You need to prep the column data for use in later analysis and remove a few intermediary columns.\n",
    "\n",
    "The `spark` context is available and `pyspark.sql.functions` is aliased as `F`. The types from `pyspark.sql.types` are already imported. The `split_df` DataFrame is as you last left it. Remember, you can use `.printSchema()` on a DataFrame in the console area to view the column names and types.\n",
    "\n",
    "⚠️ Note: If you see an `AttributeError`, refresh the exercises and click **Run Solution** without clicking **Run Code**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d64979e-d235-466d-b7d3-8c637b91436b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-----+------+----------------------------------+\n",
      "|folder  |filename       |width|height|dog_list                          |\n",
      "+--------+---------------+-----+------+----------------------------------+\n",
      "|02110627|n02110627_12938|200  |300   |[affenpinscher,0,9,173,298]       |\n",
      "|02093754|n02093754_1148 |500  |378   |[Border_terrier,73,127,341,335]   |\n",
      "|%s      |%s             |800  |600   |[Shetland_sheepdog,124,87,576,514]|\n",
      "|02104029|n02104029_63   |500  |375   |[kuvasz,0,0,499,327]              |\n",
      "|02111500|n02111500_5137 |500  |375   |[Great_Pyrenees,124,225,403,374]  |\n",
      "|02104365|n02104365_7518 |500  |333   |[schipperke,146,29,416,309]       |\n",
      "|02105056|n02105056_2834 |500  |375   |[groenendael,168,0,469,374]       |\n",
      "|02093647|n02093647_541  |500  |333   |[Bedlington_terrier,10,12,462,332]|\n",
      "|02098413|n02098413_1355 |500  |375   |[Lhasa,39,1,499,373]              |\n",
      "|02093859|n02093859_2309 |330  |500   |[Kerry_blue_terrier,17,16,300,482]|\n",
      "+--------+---------------+-----+------+----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def retriever(cols, colcount):\n",
    "    # Return a list of dog data\n",
    "    return cols[4:colcount]\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfRetriever = F.udf(retriever, ArrayType(StringType()))\n",
    "\n",
    "# Create a new column using your UDF\n",
    "split_df = split_df.withColumn('dog_list', udfRetriever(split_df['split_cols'], split_df['colcount']))\n",
    "\n",
    "# Remove the original column, split_cols, and the colcount\n",
    "split_df = split_df.drop('_c0').drop('colcount').drop('split_cols')\n",
    "\n",
    "split_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d899172-0ef5-499b-aa90-d49b540f612e",
   "metadata": {},
   "source": [
    "### Validate rows via join\n",
    "Another example of filtering data is using joins to remove invalid entries. You'll need to verify the folder names are as expected based on a given DataFrame named `valid_folders_df`. The DataFrame `split_df` is as you last left it with a group of split columns.\n",
    "\n",
    "The `spark` object is available, and `pyspark.sql.functions` is imported as `F`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8b1a596-0e8e-43ac-bc8e-0d20a5707c30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|     _c0|\n",
      "+--------+\n",
      "|02085620|\n",
      "|02085782|\n",
      "|02085936|\n",
      "|02086079|\n",
      "|02086240|\n",
      "|02086646|\n",
      "|02086910|\n",
      "|02087046|\n",
      "|02087394|\n",
      "|02088094|\n",
      "+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = f'gs://{BUCKET}/pyspark/datasets/valid_folders.txt.gz'\n",
    "# file_path = 'datasets/valid_folders.txt.gz'\n",
    "\n",
    "# Import the data to a DataFrame\n",
    "valid_folders_df = spark.read.csv(file_path)\n",
    "\n",
    "valid_folders_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a336b266-193e-43df-a72d-68cf063dbd7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 20580\n",
      "After: 19956\n"
     ]
    }
   ],
   "source": [
    "# Rename the column in valid_folders_df\n",
    "valid_folders_df = valid_folders_df.withColumnRenamed('_c0', 'folder')\n",
    "\n",
    "# Count the number of rows in split_df\n",
    "split_count = split_df.count()\n",
    "\n",
    "# Join the DataFrames\n",
    "joined_df = split_df.join(F.broadcast(valid_folders_df), \"folder\")\n",
    "\n",
    "# Compare the number of rows remaining\n",
    "joined_count = joined_df.count()\n",
    "print(\"Before: %d\\nAfter: %d\" % (split_count, joined_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22c8b80-3fe6-4279-b8b6-8b72538462c5",
   "metadata": {},
   "source": [
    "### Examining invalid rows\n",
    "You've successfully filtered out the rows using a join, but sometimes you'd like to examine the data that is invalid. This data can be stored for later processing or for troubleshooting your data sources.\n",
    "\n",
    "You want to find the difference between two DataFrames and store the invalid rows.\n",
    "\n",
    "The `spark` object is defined and `pyspark.sql.functions` are imported as `F`. The original DataFrame `split_df` and the joined DataFrame `joined_df` are available as they were in their previous states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2685f2f3-adc5-4864-b989-6bd3daca842c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " split_df:\t20580\n",
      " joined_df:\t19956\n",
      " invalid_df: \t624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 125:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 distinct invalid folders found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Determine the row counts for each DataFrame\n",
    "split_count = split_df.count()\n",
    "joined_count = joined_df.count()\n",
    "\n",
    "# Create a DataFrame containing the invalid rows\n",
    "invalid_df = split_df.join(F.broadcast(joined_df), 'folder', 'anti')\n",
    "\n",
    "# Validate the count of the new DataFrame is as expected\n",
    "invalid_count = invalid_df.count()\n",
    "print(\" split_df:\\t%d\\n joined_df:\\t%d\\n invalid_df: \\t%d\" % (split_count, joined_count, invalid_count))\n",
    "\n",
    "# Determine the number of distinct folder rows removed\n",
    "invalid_folder_count = invalid_df.select('folder').distinct().count()\n",
    "print(\"%d distinct invalid folders found\" % invalid_folder_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1715f51-73fd-401c-928e-67b8d3f61dfd",
   "metadata": {},
   "source": [
    "### Dog parsing\n",
    "You've done a considerable amount of cleanup on the initial dataset, but now need to analyze the data a bit deeper. There are several questions that have now come up about the type of dogs seen in an image and some details regarding the images. You realize that to answer these questions, you need to process the data into a specific type. Before you can use it, you'll need to create a schema / type to represent the dog details.\n",
    "\n",
    "The `joined_df` DataFrame is as you last defined it, and the `pyspark.sql.types` have all been imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f502adf-da74-4f9e-bf68-ef27ee0e87db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|dog_list                          |\n",
      "+----------------------------------+\n",
      "|[affenpinscher,0,9,173,298]       |\n",
      "|[Border_terrier,73,127,341,335]   |\n",
      "|[kuvasz,0,0,499,327]              |\n",
      "|[Great_Pyrenees,124,225,403,374]  |\n",
      "|[schipperke,146,29,416,309]       |\n",
      "|[groenendael,168,0,469,374]       |\n",
      "|[Bedlington_terrier,10,12,462,332]|\n",
      "|[Lhasa,39,1,499,373]              |\n",
      "|[Kerry_blue_terrier,17,16,300,482]|\n",
      "|[vizsla,112,93,276,236]           |\n",
      "+----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Select the dog details and show 10 untruncated rows\n",
    "print(joined_df.select('dog_list').show(10, truncate=False))\n",
    "\n",
    "# Define a schema type for the details in the dog list\n",
    "DogType = StructType([\n",
    "    StructField(\"breed\", StringType(), False),\n",
    "    StructField(\"start_x\", IntegerType(), False),\n",
    "    StructField(\"start_y\", IntegerType(), False),\n",
    "    StructField(\"end_x\", IntegerType(), False),\n",
    "    StructField(\"end_y\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1626606e-f090-4b4d-89e8-e9d16235d828",
   "metadata": {},
   "source": [
    "### Per image count\n",
    "Your next task in building a data pipeline for this dataset is to create a few analysis oriented columns. You've been asked to calculate the number of dogs found in each image based on your `dog_list` column created earlier. You have also created the `DogType` which will allow better parsing of the data within some of the data columns.\n",
    "\n",
    "The `joined_df` is available as you last defined it, and the `DogType` StructType is defined. `pyspark.sql.functions` is available under the `F` alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1461e46-dcec-4ba8-ba8c-34854854a2b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|size(dogs)|\n",
      "+----------+\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a function to return the number and type of dogs as a tuple\n",
    "def dogParse(doglist):\n",
    "    dogs = []\n",
    "    for dog in doglist:\n",
    "        (breed, start_x, start_y, end_x, end_y) = dog.split(',')\n",
    "        dogs.append((breed, int(start_x), int(start_y), int(end_x), int(end_y)))\n",
    "    return dogs\n",
    "\n",
    "# Create a UDF\n",
    "udfDogParse = F.udf(dogParse, ArrayType(DogType))\n",
    "\n",
    "# Use the UDF to list of dogs\n",
    "joined_df = joined_df.withColumn('dogs', udfDogParse('dog_list'))\n",
    "\n",
    "# Show the number of dogs in the first 10 rows\n",
    "joined_df.select(F.size('dogs')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d011d3e4-9f8b-4344-b803-2f7e5ba3af6c",
   "metadata": {},
   "source": [
    "### Percentage dog pixels\n",
    "The final task for parsing the dog annotation data is to determine the percentage of pixels in each image that represents a dog (or dogs). You'll need to use the various techniques you've learned in this course to help calculate this information and add it as columns for later analysis.\n",
    "\n",
    "To calculate the percentage of pixels, first calculate the total number of pixels representing each dog then sum them for the image. You can calculate the bounding box with the formula:\n",
    "\n",
    "(Xend - Xstart) * (Yend - Ystart)\n",
    "\n",
    "NOTE: You can ignore the possibility of overlapping bounding boxes in this instance.\n",
    "\n",
    "For the percentage, calculate the total number of \"dog\" pixels divided by the total size of the image, multiplied by 100.\n",
    "\n",
    "The `joined_df` DataFrame is as you last used it. `pyspark.sql.functions` is aliased to `F`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e9b09239-3e14-4968-93e7-c66f741998f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 140:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-----+------+--------------------+--------------------+----------+-----------------+\n",
      "|  folder|       filename|width|height|            dog_list|                dogs|dog_pixels|      dog_percent|\n",
      "+--------+---------------+-----+------+--------------------+--------------------+----------+-----------------+\n",
      "|02110627|n02110627_12938|  200|   300|[affenpinscher,0,...|[{affenpinscher, ...|     49997|83.32833333333333|\n",
      "|02104029|   n02104029_63|  500|   375|[kuvasz,0,0,499,327]|[{kuvasz, 0, 0, 4...|    163173|          87.0256|\n",
      "|02105056| n02105056_2834|  500|   375|[groenendael,168,...|[{groenendael, 16...|    112574|60.03946666666666|\n",
      "|02093647|  n02093647_541|  500|   333|[Bedlington_terri...|[{Bedlington_terr...|    144640|86.87087087087087|\n",
      "|02098413| n02098413_1355|  500|   375|[Lhasa,39,1,499,373]|[{Lhasa, 39, 1, 4...|    171120|           91.264|\n",
      "|02093859| n02093859_2309|  330|   500|[Kerry_blue_terri...|[{Kerry_blue_terr...|    131878|79.92606060606062|\n",
      "|02109961| n02109961_1017|  475|   500|[Eskimo_dog,43,20...|[{Eskimo_dog, 43,...|    189189|79.65852631578947|\n",
      "|02108000| n02108000_3491|  600|   450|[EntleBucher,307,...|[{EntleBucher, 30...|    168667|62.46925925925926|\n",
      "|02085782| n02085782_1731|  600|   449|[Japanese_spaniel...|[{Japanese_spanie...|    250125|92.84521158129176|\n",
      "|02110185| n02110185_2736|  259|   500|[Siberian_husky,7...|[{Siberian_husky,...|    113088|87.32664092664093|\n",
      "+--------+---------------+-----+------+--------------------+--------------------+----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate total pixels occupied by dogs in the image\n",
    "def dogPixelCount(doglist):\n",
    "    totalpixels = 0\n",
    "    for dog in doglist:\n",
    "        totalpixels += (dog[3] - dog[1]) * (dog[4] - dog[2])\n",
    "    return totalpixels\n",
    "\n",
    "# Define a UDF for the pixel count\n",
    "udfDogPixelCount = F.udf(dogPixelCount, IntegerType())\n",
    "\n",
    "# Add a new column 'dog_pixels' containing the pixel count for dogs in each image\n",
    "joined_df = joined_df.withColumn('dog_pixels', udfDogPixelCount('dogs'))\n",
    "\n",
    "# Add a column 'dog_percent' representing the percentage of the image occupied by dogs\n",
    "joined_df = joined_df.withColumn('dog_percent', (F.col('dog_pixels') / (F.col('width') * F.col('height'))) * 100)\n",
    "\n",
    "# Show the first 10 annotations with more than 60% dog\n",
    "joined_df.filter(F.col('dog_percent') > 60).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dea15eb-e01e-4733-bd13-d8457a655406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}